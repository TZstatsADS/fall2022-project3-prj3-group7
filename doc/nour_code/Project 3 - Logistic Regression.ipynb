{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4edaa84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-win_amd64.whl (35.6 MB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\nour\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac69ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a789ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the images\n",
    "n_img = 50000\n",
    "n_noisy = 40000\n",
    "n_clean_noisy = n_img - n_noisy\n",
    "imgs = np.empty((n_img,32,32,3))\n",
    "for i in range(n_img):\n",
    "    img_fn = f'C:/Users/nour/Downloads/train_data/images/{i+1:05d}.png'\n",
    "    imgs[i,:,:,:]=cv2.cvtColor(cv2.imread(img_fn),cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# load the labels\n",
    "clean_labels = np.genfromtxt('C:/Users/nour/Downloads/train_data/clean_labels.csv', delimiter=',', dtype=\"int8\")\n",
    "noisy_labels = np.genfromtxt('C:/Users/nour/Downloads/train_data/noisy_labels.csv', delimiter=',', dtype=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93976708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "# RGB histogram dataset construction\n",
    "no_bins = 6\n",
    "bins = np.linspace(0,255,no_bins) # the range of the rgb histogram\n",
    "target_vec = np.empty(n_img)\n",
    "feature_mtx = np.empty((n_img,3*(len(bins)-1)))\n",
    "i = 0\n",
    "for i in range(n_img):\n",
    "    # The target vector consists of noisy labels\n",
    "    target_vec[i] = noisy_labels[i]\n",
    "    \n",
    "    # Use the numbers of pixels in each bin for all three channels as the features\n",
    "    feature1 = np.histogram(imgs[i][:,:,0],bins=bins)[0] \n",
    "    feature2 = np.histogram(imgs[i][:,:,1],bins=bins)[0]\n",
    "    feature3 = np.histogram(imgs[i][:,:,2],bins=bins)[0]\n",
    "    \n",
    "    # Concatenate three features\n",
    "    feature_mtx[i,] = np.concatenate((feature1, feature2, feature3), axis=None)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d69b7726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "# Train a logistic regression model \n",
    "clf = LogisticRegression(random_state=0).fit(feature_mtx, target_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d17a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "def baseline_model(image):\n",
    "    '''\n",
    "    This is the baseline predictive model that takes in the image and returns a label prediction\n",
    "    '''\n",
    "    feature1 = np.histogram(image[:,:,0],bins=bins)[0]\n",
    "    feature2 = np.histogram(image[:,:,1],bins=bins)[0]\n",
    "    feature3 = np.histogram(image[:,:,2],bins=bins)[0]\n",
    "    feature = np.concatenate((feature1, feature2, feature3), axis=None).reshape(1,-1)\n",
    "    return clf.predict(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f47e8085",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(n_img):\n",
    "    results.append(baseline_model(imgs[i, :,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e252343d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ...,  True,  True, False]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results == clean_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1e908ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.23      0.19      4856\n",
      "           1       0.13      0.21      0.16      5070\n",
      "           2       0.12      0.03      0.05      4920\n",
      "           3       0.12      0.07      0.09      5101\n",
      "           4       0.14      0.28      0.19      5090\n",
      "           5       0.14      0.07      0.10      5049\n",
      "           6       0.15      0.20      0.17      5033\n",
      "           7       0.18      0.02      0.04      4921\n",
      "           8       0.15      0.25      0.19      5022\n",
      "           9       0.13      0.07      0.09      4938\n",
      "\n",
      "    accuracy                           0.14     50000\n",
      "   macro avg       0.14      0.14      0.13     50000\n",
      "weighted avg       0.14      0.14      0.13     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import sklearn.metrics.classification_report\n",
    "print(classification_report(noisy_labels, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb2adf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.43      0.37      1005\n",
      "           1       0.18      0.29      0.22       974\n",
      "           2       0.22      0.04      0.07      1032\n",
      "           3       0.19      0.12      0.14      1016\n",
      "           4       0.24      0.48      0.32       999\n",
      "           5       0.22      0.13      0.16       937\n",
      "           6       0.26      0.35      0.30      1030\n",
      "           7       0.29      0.04      0.07      1001\n",
      "           8       0.28      0.43      0.34      1025\n",
      "           9       0.19      0.11      0.14       981\n",
      "\n",
      "    accuracy                           0.24     10000\n",
      "   macro avg       0.24      0.24      0.21     10000\n",
      "weighted avg       0.24      0.24      0.21     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(clean_labels, results[0:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d47f344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/nour/Downloads/train_data/images/50000.png'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_fn  #imgs[i,:,:,:]=cv2.cvtColor(cv2.imread(img_fn),cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24b68b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[229, 229, 239],\n",
       "        [236, 237, 247],\n",
       "        [234, 236, 247],\n",
       "        ...,\n",
       "        [217, 219, 233],\n",
       "        [221, 223, 234],\n",
       "        [222, 223, 233]],\n",
       "\n",
       "       [[222, 221, 229],\n",
       "        [239, 239, 249],\n",
       "        [233, 234, 246],\n",
       "        ...,\n",
       "        [223, 223, 236],\n",
       "        [227, 228, 238],\n",
       "        [210, 211, 220]],\n",
       "\n",
       "       [[213, 206, 211],\n",
       "        [234, 232, 239],\n",
       "        [231, 233, 244],\n",
       "        ...,\n",
       "        [220, 220, 232],\n",
       "        [220, 219, 232],\n",
       "        [202, 203, 215]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[150, 143, 135],\n",
       "        [140, 135, 127],\n",
       "        [132, 127, 120],\n",
       "        ...,\n",
       "        [224, 222, 218],\n",
       "        [230, 228, 225],\n",
       "        [241, 241, 238]],\n",
       "\n",
       "       [[137, 132, 126],\n",
       "        [130, 127, 120],\n",
       "        [125, 121, 115],\n",
       "        ...,\n",
       "        [181, 180, 178],\n",
       "        [202, 201, 198],\n",
       "        [212, 211, 207]],\n",
       "\n",
       "       [[122, 119, 114],\n",
       "        [118, 116, 110],\n",
       "        [120, 116, 111],\n",
       "        ...,\n",
       "        [179, 177, 173],\n",
       "        [164, 164, 162],\n",
       "        [163, 163, 161]]], dtype=uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.cvtColor(cv2.imread(img_fn),cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f6e3580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[59., 62., 63.]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0,:1,:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d359fffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp39-cp39-win_amd64.whl (167.2 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nour\\anaconda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccda155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: torch==1.13.0 in c:\\users\\nour\\anaconda3\\lib\\site-packages (from torchvision) (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nour\\anaconda3\\lib\\site-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nour\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\nour\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\nour\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\nour\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nour\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nour\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nour\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bc33ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "176cbccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nour\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tqdm\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2903cd",
   "metadata": {},
   "source": [
    "running brian's code now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a6a4394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47500, 32, 32, 3), (47500,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split clean labels into training set and validation set\n",
    "\n",
    "X_train_clean_noisy, X_test_clean_noisy, y_train, y_test = train_test_split(imgs[:n_clean_noisy], np.column_stack((clean_labels, noisy_labels[:10000])), test_size=0.25,random_state=5243)\n",
    "y_train_clean = y_train[:,0]\n",
    "y_train_noisy = y_train[:,1]\n",
    "y_test_clean = y_test[:,0]\n",
    "y_test_noisy = y_test[:,1]\n",
    "if not os.path.exists(\"C:/Users/nour/data/test_labels.csv\"):\n",
    "    np.savetxt(\"C:/Users/nour/data/test_labels.csv\", y_test_clean, delimiter=\",\")\n",
    "if not os.path.exists(\"C:/Users/nour/data/test_images\"):\n",
    "    os.mkdir(\"C:/Users/nour/data/test_images\")\n",
    "    for i in range(X_test_clean_noisy.shape[0]):\n",
    "        img_fn = f'C:/Users/nour/data/test_images/test{i+1:05d}.png'\n",
    "        cv2.imwrite(img_fn,cv2.cvtColor(X_test_clean_noisy[i].astype('float32'), cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "X_train_clean_noisy.shape\n",
    "X_train = np.concatenate((X_train_clean_noisy,imgs[10000:]))\n",
    "y_train_total_noisy = np.concatenate((y_train_noisy,noisy_labels[10000:]))\n",
    "X_train.shape, y_train_total_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0fccef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_prepocessing(image,transform):\n",
    "    return torch.unsqueeze(transform(image.astype(np.uint8)),0).float()\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Model1Dataset(Dataset):\n",
    "    def __init__(self,X_train,y_train,transform=None):\n",
    "        self.X = X_train\n",
    "        self.y = y_train\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x = self.X[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x.astype(np.uint8))   \n",
    "        else:\n",
    "            x = np.transpose(x,(2,0,1))\n",
    "        return x, self.y[idx]\n",
    "    \n",
    "# Image Augmentation\n",
    "transform_model1 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "#     transforms.Resize((227,227)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "])\n",
    "\n",
    "# model1_dataset = Model1Dataset(imgs,noisy_labels,transform=transform)\n",
    "model1_dataset = Model1Dataset(X_train,y_train_total_noisy,transform=transform_model1) #I assume he meant transform_model1?\n",
    "# model1_dataloader = DataLoader(model1_dataset, batch_size=128, shuffle=True)\n",
    "model1_dataloader = DataLoader(model1_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "###what does this do?\n",
    "model1 = nn.Sequential(\n",
    "    LeNet(),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec03faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model1(dataloader,model,loss_fn, optimizer, epochs, filename):\n",
    "    for i in range(epochs):\n",
    "        train_loss, train_acc,n = 0, 0, 0\n",
    "        for X,y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(X.float())\n",
    "            loss = loss_fn(y_hat,y.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            n += 1\n",
    "        print(f\"{i+1} epoches, train loss:{train_loss/n}\")\n",
    "    torch.save(model.state_dict(), filename)\n",
    "train_model1(model1_dataloader,model1,loss_fn,optimizer,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e4c9d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001E3728FC6D0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "433cdb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LeNet(\n",
       "    (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  )\n",
       "  (1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model1 = model1\n",
    "new_model1.load_state_dict(torch.load('project3_model1'))\n",
    "new_model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ced5cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [BUILD A MORE SOPHISTICATED PREDICTIVE MODEL]\n",
    "\n",
    "# write your code here...\n",
    "        \n",
    "def model_I(image):\n",
    "    '''\n",
    "    This function should takes in the image of dimension 32*32*3 as input and returns a label prediction\n",
    "    '''\n",
    "\n",
    "    probs = new_model1(image_prepocessing(image,transform_model1))\n",
    "    pred = np.argmax(probs.detach().numpy(),axis=1) \n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f022329c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_I(imgs[1231])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "462fac67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[1231].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90756ce3",
   "metadata": {},
   "source": [
    "# model 1 with corrected labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "de203379",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [117]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model1_correct \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      2\u001b[0m     LeNet(),\n\u001b[0;32m      3\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_model1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel1_correct\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel1_corrected_labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [87]\u001b[0m, in \u001b[0;36mtrain_model1\u001b[1;34m(dataloader, model, loss_fn, optimizer, epochs, filename)\u001b[0m\n\u001b[0;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 13\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_hat,y\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36mLeNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# flatten all dimensions except batch\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 512]"
     ]
    }
   ],
   "source": [
    "class LeNetCorrected(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "model1_correct = nn.Sequential(\n",
    "    LeNetCorrected(),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n",
    "train_model1(model2_dataloader,model1_correct,loss_fn,optimizer,epochs,'model1_corrected_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6770d",
   "metadata": {},
   "source": [
    "more sophisticated model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ebc91a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nour\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nour\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\nour/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c351c21e1fc4dc5bb04857bf63e28b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Pre-trained Alexnet as image encoder \n",
    "img_encoder = models.resnet18(pretrained=True) \n",
    "img_encoder.fc = nn.Identity()\n",
    "img_encoder.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11df3881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 47500/47500 [48:28<00:00, 16.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47500, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform_model2 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((227,227)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "])\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     encoded_imgs = np.empty((n_img,512))\n",
    "#     for i in tqdm.tqdm(range(n_img)):\n",
    "#         encoded_imgs[i] = img_encoder(image_prepocessing(imgs[i],transform_model2))\n",
    "        \n",
    "with torch.no_grad():\n",
    "    encoded_imgs = np.empty((X_train.shape[0],512))\n",
    "    for i in tqdm.tqdm(range(X_train.shape[0])):\n",
    "        encoded_imgs[i] = img_encoder(image_prepocessing(X_train[i],transform_model2))\n",
    "        \n",
    "print(encoded_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e29be0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('encoded_imgs.txt', encoded_imgs)\n",
    "# encoded_imgs = np.loadtxt('encoded_imgs.txt')\n",
    "\n",
    "np.savetxt('encoded_imgs_47500.txt', encoded_imgs)\n",
    "encoded_imgs = np.loadtxt('encoded_imgs_47500.txt')\n",
    "\n",
    "encoded_imgs.shape\n",
    "\n",
    "class Model2LabelDataset(Dataset):\n",
    "    def __init__(self,X_train_clean_noisy,y_train_noisy,y_train_clean):\n",
    "        self.X = X_train_clean_noisy\n",
    "        self.y_clean = y_train_clean\n",
    "        self.y_noisy = y_train_noisy\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.y_noisy[idx],self.y_clean[idx]\n",
    "    \n",
    "# model2_label_dataset = Model2LabelDataset(encoded_imgs[:10000],noisy_labels[:10000],clean_labels)\n",
    "# model2_label_dataloader = DataLoader(model2_label_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model2_label_dataset = Model2LabelDataset(encoded_imgs[:7500],y_train_noisy,y_train_clean)\n",
    "model2_label_dataloader = DataLoader(model2_label_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "class Model2LabelClean(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(Model2LabelClean,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.fc1 = nn.Linear(512,100)\n",
    "        self.fc2 = nn.Linear(self.num_classes,28)\n",
    "        self.fc3 = nn.Linear(128,64)\n",
    "        self.fc4 = nn.Linear(64,self.num_classes)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.bn2 = nn.BatchNorm1d(28)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    # x: Image\n",
    "    # y: Label\n",
    "    def forward(self,x,noisy_label):\n",
    "        noisy_label = nn.functional.one_hot(noisy_label.long(), num_classes=self.num_classes)\n",
    "        x = self.bn1(nn.functional.relu(self.fc1(x)))\n",
    "        y_noisy = self.bn2(nn.functional.relu(self.fc2(noisy_label.float())))\n",
    "        c = torch.cat((x,y_noisy),axis=1)\n",
    "        c = self.bn3(nn.functional.relu(self.fc3(c)))\n",
    "        c = torch.clip(self.fc4(c) + noisy_label,0,1)\n",
    "        return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75299944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoches, train loss:1.8028108174518003\n",
      "1 epoches, train loss:1.6828716740769856\n",
      "2 epoches, train loss:1.644332543268042\n",
      "3 epoches, train loss:1.6213750728106096\n",
      "4 epoches, train loss:1.6137569112292791\n",
      "5 epoches, train loss:1.5964474708346996\n",
      "6 epoches, train loss:1.5907055604255806\n",
      "7 epoches, train loss:1.5840460779303212\n",
      "8 epoches, train loss:1.580139677403337\n",
      "9 epoches, train loss:1.5744782181109411\n"
     ]
    }
   ],
   "source": [
    "model2_label_clean = Model2LabelClean(10)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(model2_label_clean.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model2_label(dataloader,model,loss_fn, optimizer, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_loss, train_acc,n = 0, 0, 0\n",
    "        for X,y_noisy,y_clean in dataloader:\n",
    "            y_hat = model(X.float(),y_noisy)\n",
    "            loss = loss_fn(y_hat,y_clean.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            n += 1\n",
    "        print(f\"{i} epoches, train loss:{train_loss/n}\")\n",
    "    torch.save(model.state_dict(), 'project3_model2_label_clean')\n",
    "train_model2_label(model2_label_dataloader,model2_label_clean,loss_fn,optimizer,epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eced654f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model2LabelClean(\n",
       "  (fc1): Linear(in_features=512, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=28, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model2_label_clean = model2_label_clean\n",
    "new_model2_label_clean.load_state_dict(torch.load('project3_model2_label_clean'))\n",
    "new_model2_label_clean.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b7b4f202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 9 7 3 0 0 4 0 3] [4 7 8 2 4 2 6 8 4 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e372e750a0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = model2_label_clean(torch.tensor(encoded_imgs[30000:30010]).float(),torch.tensor(noisy_labels[30000:30010]))\n",
    "print(np.argmax(res.detach().numpy(),axis=1) ,noisy_labels[30000:30010])\n",
    "plt.imshow(imgs[30000]/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa25698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47500,)\n"
     ]
    }
   ],
   "source": [
    "# predicted_clean_probs= model2_label_clean(torch.tensor(encoded_imgs[10000:]).float(),torch.tensor(noisy_labels[10000:]))\n",
    "predicted_clean_probs= model2_label_clean(torch.tensor(encoded_imgs[7500:]).float(),torch.tensor(noisy_labels[10000:]))\n",
    "predicted_clean_labels = np.argmax(predicted_clean_probs.detach().numpy(),axis=1) \n",
    "# model2_labels = np.concatenate((clean_labels, predicted_clean_labels),axis=0)\n",
    "model2_labels = np.concatenate((y_train_clean, predicted_clean_labels),axis=0)\n",
    "print(model2_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ab5548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2Dataset(Dataset):\n",
    "    def __init__(self,X_train,y_train):\n",
    "        self.X = X_train\n",
    "        self.y = y_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "model2_dataset = Model2Dataset(encoded_imgs,model2_labels)\n",
    "model2_dataloader = DataLoader(model2_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb5b4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(Model2,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.fc1 = nn.Linear(512,256)\n",
    "        self.fc2 = nn.Linear(256,64)\n",
    "        self.fc3 = nn.Linear(64,self.num_classes)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    # x: Image\n",
    "    # y: Label\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(nn.functional.relu(self.fc1(x)))\n",
    "        x = self.bn2(nn.functional.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9bb52e24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoches, train loss:0.7119387150291474\n",
      "2 epoches, train loss:0.5475597585080773\n",
      "3 epoches, train loss:0.5233229605420944\n",
      "4 epoches, train loss:0.5059980606680275\n",
      "5 epoches, train loss:0.4947857740585522\n",
      "6 epoches, train loss:0.48200118101091793\n",
      "7 epoches, train loss:0.47306936282304024\n",
      "8 epoches, train loss:0.46545149450020123\n",
      "9 epoches, train loss:0.4606695357029156\n",
      "10 epoches, train loss:0.4482229122070856\n"
     ]
    }
   ],
   "source": [
    "model2 = Model2(10)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model2(dataloader,model,loss_fn, optimizer, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_loss, train_acc,n = 0, 0, 0\n",
    "        for X,y in dataloader:\n",
    "            y_hat = model(X.float())\n",
    "            loss = loss_fn(y_hat,y.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            n += 1\n",
    "        print(f\"{i+1} epoches, train loss:{train_loss/n}\")\n",
    "    torch.save(model.state_dict(), 'project3_model2')\n",
    "train_model2(model2_dataloader,model2,loss_fn,optimizer,epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "793f60a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model2LabelClean(\n",
       "  (fc1): Linear(in_features=512, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=28, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model2_label_clean = model2_label_clean\n",
    "new_model2_label_clean.load_state_dict(torch.load('project3_model2_label_clean'))\n",
    "new_model2_label_clean.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "96423974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3968"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(len(clean_labels)):\n",
    "    if clean_labels[i] == noisy_labels[i]:\n",
    "        acc += 1\n",
    "acc / len(clean_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a651a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [0 for i in range(10)]\n",
    "for i in range(len(clean_labels)):\n",
    "    if clean_labels[i] == noisy_labels[i]:\n",
    "        acc += 1\n",
    "acc / len(clean_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8f64d5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(clean_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46c16bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clean = [0] * 10\n",
    "num_same = [0] * 10\n",
    "for i in range(len(clean_labels)):\n",
    "    num_clean[clean_labels[i]] += 1\n",
    "    if clean_labels[i] == noisy_labels[i]:\n",
    "        num_same[clean_labels[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "622eb7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1005\n",
      "1 974\n",
      "2 1032\n",
      "3 1016\n",
      "4 999\n",
      "5 937\n",
      "6 1030\n",
      "7 1001\n",
      "8 1025\n",
      "9 981\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i, num_clean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "77128ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 6, 3, ..., 9, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_clean_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291dba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f07ae508",
   "metadata": {},
   "source": [
    "feature extraction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a422c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 47500/47500 [07:43<00:00, 102.37it/s]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Image Augmentation\n",
    "transform_model1_new = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "#     transforms.Resize((227,227)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "])\n",
    "\n",
    "# model1_dataset = Model1Dataset(imgs,noisy_labels,transform=transform)\n",
    "#model1_dataset_new = Model1Dataset(X_train,y_train_total_noisy,transform=transform_model1) #I assume he meant transform_model1?\n",
    "# model1_dataloader = DataLoader(model1_dataset, batch_size=128, shuffle=True)\n",
    "#model1_dataloader_new = DataLoader(model1_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "###what does this do?\n",
    "lenet = LeNet()\n",
    "\n",
    "#train_model1(model1_dataloader,model1,loss_fn,optimizer,epochs)\n",
    "\n",
    "# Will contain the feature\n",
    "features = []\n",
    "\n",
    "# Iterate each image\n",
    "for i in tqdm.tqdm(range(X_train.shape[0])):\n",
    "    path = f'C:/Users/nour/Downloads/train_data/images/{i+1:05d}.png'\n",
    "  # Read the file\n",
    "    img_new = cv2.imread(path)\n",
    "    img_new = transform_model1_new(img_new)\n",
    "    # Reshape the image. PyTorch model reads 4-dimensional tensor\n",
    "    # [batch_size, channels, width, height]\n",
    "    #img_new = img_new.reshape(1, 6, 16, 5)\n",
    "    img_new = img_new.to(device)\n",
    "    # We only extract features, so we don't need gradient\n",
    "    with torch.no_grad():\n",
    "    # Extract the feature from the image\n",
    "        feature = lenet(img_new)\n",
    "    # Convert to NumPy Array, Reshape it, and save it to features variable\n",
    "    features.append(feature.cpu().detach().numpy().reshape(-1))\n",
    "\n",
    "# Convert to NumPy Array\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5283f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 0 ... 1 3 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize the model\n",
    "k_model = KMeans(n_clusters=5, random_state=42)\n",
    "\n",
    "# Fit the data into the model\n",
    "k_model.fit(features)\n",
    "\n",
    "# Extract the labels\n",
    "k_labels = k_model.labels_\n",
    "\n",
    "print(k_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "95dd0202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 47500)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_clean), len(k_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d3203111",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = [[]] * 10\n",
    "count = [0] * 10\n",
    "for i in range(7500):\n",
    "    count[y_train_clean[i]] += 1\n",
    "    lis[y_train_clean[i]].append(k_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a5cacac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.8153333333333332\n",
      "1 1.8153333333333332\n",
      "2 1.8153333333333332\n",
      "3 1.8153333333333332\n",
      "4 1.8153333333333332\n",
      "5 1.8153333333333332\n",
      "6 1.8153333333333332\n",
      "7 1.8153333333333332\n",
      "8 1.8153333333333332\n",
      "9 1.8153333333333332\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i, sum(lis[i])/len(lis[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a8c6a069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Model2Dataset at 0x1e3a2d992e0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5efa6a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e7c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
